# Portfolio of Chemical Data Integration Projects Using FAIR Principles

This report presents four case study projects demonstrating the integration of chemical datasets using Python-based pipelines, semantic web technologies (RDF and graph databases), and the FAIR data principles[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1")[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://knowledgebase.nfdi4chem.de/knowledge_base/docs/fair/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "2"). Each project corresponds to a specific theme and is designed as a fully reproducible portfolio project. The projects include: **(1)** Public Reference & Regulatory Datasets, **(2)** Chemical & Formulation Property Repositories, **(3)** In-House & Synthetic Manufacturing Data, and **(4)** Linking Across Datasets. For each project, we outline the objectives, data sources, methodology (pipeline for data ingestion, transformation, and visualization), tools and technologies used, and expected outcomes. All projects emphasize **FAIR** (Findable, Accessible, Interoperable, Reusable) data practices—such as using unique identifiers, rich metadata, and standard formats—to ensure the resulting data assets are well-documented and reusable[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1")[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://knowledgebase.nfdi4chem.de/knowledge_base/docs/fair/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "2"). Inline citations to a wide range of web sources are provided throughout (no separate reference list is included), to validate the approach and techniques used.

---

## Project 1: Public Reference & Regulatory Chemical Dataset Integration

### Objectives  
This project focuses on building a knowledge base from **public reference and regulatory chemical datasets**. The goal is to integrate widely available chemical information (e.g. identifiers, structures, properties) with regulatory data (e.g. hazard classifications, legal status) into a single searchable platform. Key objectives include: 

- **Consolidation of Public Data:** Ingest data from major public chemical databases and regulatory sources (for example, the European Chemicals Agency’s database and EPA’s chemical inventories) into a unified format[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://polymer-additives.specialchem.com/news/industry-news/echa-chem-new-database-reach-eu-000233041?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "3")[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://comptox.epa.gov/dashboard/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "4").  
- **FAIR Data Curation:** Transform and annotate the data using semantic standards (RDF triples with ontologies) so that each chemical and its attributes have persistent URIs and rich metadata, ensuring findability and interoperability[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1")[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://knowledgebase.nfdi4chem.de/knowledge_base/docs/fair/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "2").  
- **Regulatory Insight:** Enable queries that cross-reference regulatory information with chemical properties. For instance, users should be able to retrieve all substances classified as hazardous (e.g. carcinogenic) along with their physical constants or usage restrictions[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.prc.cnrs.fr/reach/en/databases.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "5").  
- **Documentation & Reproducibility:** Provide clear instructions and scripts for fetching data, converting formats, loading into a graph database, and running example queries or visualizations so that others can reproduce the entire pipeline.

### Data Sources and Tools Used  
**Key Datasets:** The project draws on several authoritative public datasets to gather chemical and regulatory information: 

| Dataset Source                      | Contents & Relevance                                             | Access Method                            |
|-------------------------------------|------------------------------------------------------------------|------------------------------------------|
| **ECHA CHEM (EU)**[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://polymer-additives.specialchem.com/news/industry-news/echa-chem-new-database-reach-eu-000233041?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "3")      | Over 100,000 REACH-registered chemicals with industry-submitted data (identifiers, hazards, phys-chem properties, etc.)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://polymer-additives.specialchem.com/news/industry-news/echa-chem-new-database-reach-eu-000233041?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "3")[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.prc.cnrs.fr/reach/en/databases.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "5"). Also includes classification & labelling inventory and regulatory lists. | ECHA Open Data API / download (JSON/XML)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://api.store/eu-institutions-api/european-chemicals-agency-api?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "6") |
| **EPA CompTox Dashboard (US)**[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://comptox.epa.gov/dashboard/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "4") | >1.2 million chemicals with integrated data on substances, environmental and toxicity info, and regulatory flags[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://comptox.epa.gov/dashboard/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "4"). Serves as a hub for US regulatory data (e.g. TSCA inventory, DSSTox IDs). | Dashboard search API and DSSTox data files (CSV/TSV) |
| **PubChem** (NCBI)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01017-0?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "7")     | Open chemical database with structures, identifiers (CAS, InChIKey), and various annotated properties. Used as a reference for chemical identifiers and cross-links. | PUG REST API for compound data (JSON/CSV)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://cran.r-project.org/web//packages//PubChemR/vignettes/Exploring_Chemical_Data_with_PubChemR.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "8") |
| **Other Reference Data**           | E.g. NIST Chemistry WebBook or OECD eChemPortal for standard reference properties and international regulatory data. These provide supplemental properties or hazard data not in the above. | Bulk downloads or web services (where available) |

**Tools & Technologies:** The implementation uses a combination of Python libraries for data handling, RDF processing for semantic modeling, and a graph database for storage and querying: 

| Tool / Technology        | Purpose in Project 1                                   |
|--------------------------|--------------------------------------------------------|
| **Python + Pandas**      | General data ingestion and cleaning (e.g. reading CSV/JSON from APIs, normalizing field names and units). Pandas helps transform tabular regulatory data into a consistent structure for RDF conversion[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1"). |
| **Requests/urllib**      | Accessing web APIs and downloading datasets. For example, calling ECHA’s open data API or PubChem’s REST service to fetch chemical records in JSON[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://cran.r-project.org/web//packages//PubChemR/vignettes/Exploring_Chemical_Data_with_PubChemR.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "8"). |
| **RDFlib (Python)**[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://rdflib.readthedocs.io/en/stable/gettingstarted.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "9") | Converting data into RDF triples and creating a knowledge graph. RDFlib lets us define a graph and add triples like `(ChemicalX, hasProperty, FlashPoint)` in Python[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://rdflib.readthedocs.io/en/stable/gettingstarted.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "9"). We use built-in RDFLib methods to serialize data to Turtle or JSON-LD. |
| **Ontologies/Vocabularies** | Standard schemas to define data meaning. We use existing ontologies where possible: e.g. FOAF/Schema.org for basic metadata, the Chemical Entities of Biological Interest (ChEBI) ontology for substances, and industry terms for hazards (GHS classes). This ensures data fields (like “boiling point” or “carcinogenicity”) use shared definitions[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1"). |
| **Graph Database (Triple Store)** | An RDF-compatible graph database (e.g. Apache Jena Fuseki or GraphDB) is used to load the triples and enable SPARQL querying[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://aws.amazon.com/blogs/database/triple-your-knowledge-graph-speed-with-rdf-linked-data-and-opencypher-using-amazon-neptune-analytics/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "10"). This database will store the integrated knowledge graph of chemicals. We ensure it supports SPARQL 1.1 for complex queries (e.g. joins between chemical and regulatory data). |
| **SPARQL Query Interface** | For analysis and visualization, SPARQL is used to retrieve data from the graph (e.g. list all substances with certain hazard codes). The endpoint can be accessed via a Jupyter notebook (using RDFLib’s SPARQL wrapper or SPARQLWrapper library) to fetch results for further analysis/plotting. |
| **Visualization**        | Basic charts (with Matplotlib/Seaborn) to summarize data trends, and optionally network visualization (using NetworkX or a web-based graph viewer) to illustrate linkages between chemicals and regulatory categories. |

### Methodology and Pipeline  
The pipeline is structured as a sequence of steps, from data retrieval to knowledge graph construction and analysis. Each step is executed with reproducible code (provided in a Jupyter notebook or Python scripts), and the environment (Python libraries, versions) is documented. Below is the workflow outline:

| Step  | Process Description                                   | Implementation Details and Tools             |
|-------|--------------------------------------------------------|----------------------------------------------|
| **1. Data Ingestion**  | *Acquire public data from sources.* The pipeline begins by downloading or querying the public datasets. For instance, a script retrieves a list of chemicals and their classifications from ECHA’s API and pulls basic compound info from PubChem for each chemical (to get standardized identifiers like InChIKey). Similarly, EPA’s DSSTox/CompTox data (which often includes CAS and properties) is loaded. | Python with `requests` to call REST endpoints (e.g. ECHA’s API). Responses (JSON or CSV) are parsed using `json` module or `pandas.read_csv`. The code throttles requests to respect API limits[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://cran.r-project.org/web//packages//PubChemR/vignettes/Exploring_Chemical_Data_with_PubChemR.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "8"). All raw data files are saved to a `data/raw/` folder for provenance. |
| **2. Data Cleaning & Mapping**  | *Normalize and integrate data fields.* Next, the script cleans the data: handling missing values, standardizing units (e.g. all temperatures in °C), and ensuring consistent identifiers. Each chemical is assigned a unique identifier in the graph (for example, using the InChIKey as the URI part, or a namespace like `http://example.org/chem/{CASRN}`). Regulatory data (like hazard classes) are mapped to controlled vocabularies. For example, the hazard code “H350” (May cause cancer) is mapped to an ontology term `eg:Carcinogenic` in our schema. | **Pandas** is used heavily here to manipulate dataframes of properties. A lookup table is used for unit conversions (ensuring values are comparable). We utilize **RDKit** (chemistry toolkit) to generate InChIKeys from chemical names or SMILES when needed, ensuring consistent keys for linking. We then prepare RDF triples: e.g. for each chemical, create triples for its name, identifiers, and each property or classification. RDFLib helps define namespaces and add triples programmatically[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://rdflib.readthedocs.io/en/stable/gettingstarted.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "9"). |
| **3. RDF Graph Construction**  | *Convert tabular data to an RDF knowledge graph.* Using the cleaned data, we instantiate an RDF graph. Each chemical becomes a subject node with predicates for its attributes. For example, `:Chemical_X rdf:type :ChemicalSubstance ; dcterms:identifier "CAS123-45-6" ; chem:hasMeltingPoint "273.15 K"^^xsd:double .` We also encode relationships, e.g., linking a chemical to a hazard classification resource: `:Chemical_X ex:hasHazard :Hazard_Carcinogenic.` If the source data provides URLs (like ECHA’s website URI for the substance), those are used as identifiers (to directly link to their source and maintain provenance). | **Ontology Design:** We define or reuse classes and properties: e.g., a class `ChemicalSubstance` and properties like `hasProperty`, `hasClassification`. Standard vocabularies (Dublin Core for references, SKOS for classification schemes) are employed for interoperability[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1"). **RDFlib** is used to add triples for each row of data[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://rdflib.readthedocs.io/en/stable/gettingstarted.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "9"). We then serialize the graph to a Turtle file (`data/chem_regulatory.ttl`). Each triple store entry includes source attribution via properties like `prov:wasDerivedFrom` linking to the original dataset source (satisfying provenance needs of FAIR[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1")). |
| **4. Loading into Graph DB**  | *Populate the graph database with RDF data.* The Turtle (or N-Triples) file is loaded into the chosen triple store. This makes all data queryable via SPARQL. If using a cloud service (e.g. Amazon Neptune or an open source GraphDB/Fuseki), we either use their bulk loader or a SPARQL `INSERT` script. On success, the knowledge graph contains an interconnected dataset of chemicals. Because RDF inherently links data by shared nodes, any chemical present in multiple sources merges at the node (e.g. the same InChIKey from PubChem and ECHA will refer to one node)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://aws.amazon.com/blogs/database/triple-your-knowledge-graph-speed-with-rdf-linked-data-and-opencypher-using-amazon-neptune-analytics/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "10"). | The **graph database** (e.g. GraphDB) is configured and the RDF file imported through its workbench. We verify that key entities (chemicals) are consolidated rather than duplicated – if needed, we declare `owl:sameAs` between equivalent URIs from different sources to explicitly merge identities[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://aws.amazon.com/blogs/database/triple-your-knowledge-graph-speed-with-rdf-linked-data-and-opencypher-using-amazon-neptune-analytics/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "10"). The triple store’s indexing ensures the data is **findable** via its SPARQL endpoint[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1"). |
| **5. Querying & Analysis**  | *Perform example queries and visualize results.* With the knowledge graph in place, we conduct several use-case analyses to demonstrate utility. For example, a SPARQL query finds all substances classified as “very toxic to aquatic life” and retrieves their vapor pressure and water solubility values (to examine if low-solubility correlates with high hazard). Another query might join internal regulatory lists with external identifiers – e.g., listing chemicals that are on a restriction list in the EU and seeing if they appear in the EPA’s inventory. Results are then processed for visualization or summary statistics. | **SPARQL**: We run queries in a Jupyter notebook using a SPARQL client. One query uses a `FILTER` to find chemicals with certain hazard codes; another does a `JOIN` (by shared chemical URI) between properties and regulation data. **Visualization**: The returned dataframes are plotted. For instance, a bar chart shows the count of chemicals per hazard category, or a scatter plot of flash point vs. toxicity rating to find any trends. We also use a simple **network graph visualization** to depict a small subgraph: nodes for a chemical, linking to nodes of regulatory classifications and property values (converted to nodes or shown as labels). |

All steps above are documented in the project’s README and the code comments. The environment (e.g. `requirements.txt` listing Python packages and versions) is provided to ensure reproducibility. Running the pipeline yields the integrated RDF dataset and a set of example query outputs for verification.

### Expected Outcomes and Results  
By the end of Project 1, we achieve a **unified semantic dataset** of chemical information enriched with regulatory context. The expected outcomes include:

- **Integrated Knowledge Graph:** A graph database containing nodes for each chemical substance, connected to their properties and regulatory classifications. The use of RDF and common ontologies means this dataset is interoperable and can be expanded or linked to other RDF datasets easily[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://aws.amazon.com/blogs/database/triple-your-knowledge-graph-speed-with-rdf-linked-data-and-opencypher-using-amazon-neptune-analytics/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "10"). For example, if another agency publishes data in RDF, it can be loaded and auto-linked via shared identifiers (linked data network)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://aws.amazon.com/blogs/database/triple-your-knowledge-graph-speed-with-rdf-linked-data-and-opencypher-using-amazon-neptune-analytics/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "10"). The data is **FAIR**: findable via unique URIs and SPARQL queries, accessible through the open SPARQL endpoint (or downloadable TTL file), interoperable via standard RDF/OWL, and reusable with provenance and documentation[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1"). 
- **Reproducible Pipeline:** A fully documented workflow that others can follow to regenerate or update the knowledge graph. For instance, if ECHA releases an update, one can rerun the ingestion script to fetch new data. The pipeline can be adapted to other domains, since it demonstrates how to turn raw CSV/JSON into a semantic graph with Python. This aligns with the notion that well-curated research data (with scripts) makes researchers’ tasks easier and enables reuse[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://knowledgebase.nfdi4chem.de/knowledge_base/docs/fair/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "2").
- **Insights via Queries:** The project delivers sample analyses that demonstrate the power of linking datasets. For example, one result might be a table of chemicals that have high production volume (from regulatory registration data) yet lack certain hazard classifications, highlighting candidates for further review. Another result could be a list of substances flagged by both EU and US regulators (using the combined data) – effectively showing intersection of regulatory lists. These examples illustrate answering questions that would be difficult without integrating the data sources.
- **Visualization of Regulatory Landscape:** Some illustrative plots or graphs are produced, such as a pie chart of chemical hazard classes frequency in the dataset (e.g. percentage of substances labeled flammable, toxic, etc.) based on the integrated data[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.prc.cnrs.fr/reach/en/databases.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "5"). A network diagram may show a subgraph where a particular chemical is linked to multiple regulatory categories and data points, giving a quick visual cue of how interconnected the information is. 

Overall, Project 1 provides a solid foundation for managing public chemical data. It shows how **multiple open datasets can be combined into a knowledge graph** to enable richer queries and insights than any single source alone[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://aws.amazon.com/blogs/database/triple-your-knowledge-graph-speed-with-rdf-linked-data-and-opencypher-using-amazon-neptune-analytics/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "10"). This project’s outputs will also serve as building blocks for Project 4, where we link across even more datasets including internal data.

---

## Project 2: Chemical & Formulation Property Repository

### Objectives  
Project 2 involves creating a **repository of chemical and formulation properties** – essentially a centralized database (and knowledge graph) of various physical, chemical, and formulation-related properties for substances and mixtures. The objectives include:

- **Aggregating Diverse Property Data:** Collect data on properties such as melting/boiling points, solubility, viscosity, stability, etc., from multiple sources (public databases, literature, or corporate sources). This covers pure compounds and formulated mixtures (e.g. product formulations or multi-component systems).  
- **FAIR Structuring of Properties:** Standardize how properties are represented and annotated – for example, ensuring each property has metadata (units, conditions, data source) and using a consistent schema so that data can be easily merged or compared[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://pypi.org/project/chemicals/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "11")[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://knowledgebase.nfdi4chem.de/knowledge_base/docs/fair/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "2"). Use RDF/graph model to capture relationships between **components** of formulations and their measured outcomes.  
- **Tool Demonstration:** Showcase use of Python tools and scientific libraries to manage chemical property data. For instance, demonstrate using an existing open chemical property library or API (like PubChem or an open thermodynamics database) to fetch data programmatically[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://pypi.org/project/chemicals/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "11")[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://cran.r-project.org/web//packages//PubChemR/vignettes/Exploring_Chemical_Data_with_PubChemR.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "8").  
- **Reproducible Analysis of Formulations:** Provide a clear pipeline for ingesting a set of formulation recipes (chemical mixtures with ingredients and their ratios) and linking them to known component properties. The outcome should allow querying, for example: “Find all formulations containing a specific ingredient and compare their shelf-life or performance properties,” highlighting how integrated data can yield insights for product development.  
- **Documentation & Tutorials:** Ensure the repository build process is documented and includes examples on how to add new data or retrieve information (e.g. how to add a new property type, or how to use a SPARQL query to get all density measurements for a certain chemical).

### Data Sources and Tools Used  
**Key Data Sources:** The property repository draws from both **pure substance data** and **formulation (mixture) data**:

| Dataset / Source                           | Description & Relevance                                              | Access |
|--------------------------------------------|-----------------------------------------------------------------------|--------|
| **PubChem & NIST Data**[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://cran.r-project.org/web//packages//PubChemR/vignettes/Exploring_Chemical_Data_with_PubChemR.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "8")      | Large-scale sources of pure compound properties. PubChem contains experimental and predicted properties for millions of compounds (like melting points, logP, etc.), accessible via PUG REST[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://cran.r-project.org/web//packages//PubChemR/vignettes/Exploring_Chemical_Data_with_PubChemR.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "8"). NIST provides high-quality thermochemical data (heats of formation, phase change data, etc.) for common substances. These sources ensure each chemical’s basic properties are well documented. | PubChem PUG REST (programmatic JSON/CSV); NIST web queries or SRD data files. |
| **Chemicals Python Library (ChEDL)**[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://pypi.org/project/chemicals/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "11") | An open-source compiled database of chemical constants and functions (covers ~20,000 chemicals)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://pypi.org/project/chemicals/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "11"). This library provides quick access to data like normal boiling point, critical pressure, toxicity information, etc., via simple Python calls[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://pypi.org/project/chemicals/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "11"). It serves as a local reference for properties, which can be seeded into our repository. | Accessed via Python (`pip install chemicals`), data loaded on-demand from the library’s internal databanks[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://pypi.org/project/chemicals/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "11"). |
| **Formulation Datasets (Public/Custom)** | Data on multi-component mixtures. For example, open research datasets such as those compiled in the *CheMixHub* project, which gathered ~500k data points for chemical mixture properties across formulations (drug delivery, battery electrolyte, etc.)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://arxiv.org/abs/2506.12231?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "12"). These data include how combinations of chemicals behave, which is key for formulation science. Additionally, example formulation data can come from patents or publications (e.g. cosmetic formulations, polymer blends) that list ingredients and efficacy metrics. | CheMixHub data (if accessible via their repository or supplemental files); or manual curation from literature (entered into CSV/JSON). |
| **Corporate Formulation Records** (simulated) | For a realistic scenario, we include a mock in-house dataset of formulations – e.g. a CSV of paint formulations with ingredients and measured properties (viscosity, drying time, etc.), or pharmaceutical formulations with their stability results. This represents data that a company formulation lab might have. | Local CSV/Excel (as an example input which the pipeline will parse). |

**Tools & Technologies:** 

| Tool / Technology            | Role in Project 2                                           |
|------------------------------|-------------------------------------------------------------|
| **Python (pandas, numpy)**   | Core for data manipulation. For example, reading property tables (CSV/JSON) into pandas DataFrames, merging datasets on common identifiers (chemical name or CAS number), and computing any derived metrics (like mixture averages). **NumPy** might be used for calculations (e.g. computing mixture properties from components). |
| **Chemistry Libraries**      | **Chemicals (PyPI)** is used to fetch pure component data in code (as a supplement to external data)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://pypi.org/project/chemicals/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "11"). Additionally, **RDKit** is utilized to handle chemical identifiers – e.g. converting between names, SMILES, and InChIKeys for consistency. This helps match ingredients in formulation data to the pure compound data by a unique key. |
| **RDFLib and Ontologies**    | As in Project 1, RDFLib is used to model the data as an RDF graph. We define an ontology for formulations: e.g. a class `Formulation` that has relationships to multiple `ChemicalSubstance` entities (its components), and has properties like `hasProperty`->`PropertyValue`. We reuse existing vocabularies where possible: e.g. schema.org’s *Recipe* pattern as an analogy (with ingredients), or the QUDT ontology for units of measure (ensuring each property value carries a unit tag). Each property (density, pH, etc.) is an entity or well-defined predicate so that it can be queried uniformly across substances[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1"). |
| **Graph Database (Triplestore)** | The RDF graph of properties and formulations is stored in a graph database (could be the same as Project 1’s or separate). Storing here allows linking substances to their properties and also linking formulations to their constituent substances in a queryable way. For example, in SPARQL one can traverse `Formulation -> contains -> Chemical -> hasProperty -> value`. The graph DB also ensures data is accessible via SPARQL and can be merged with other graphs later (Project 4). |
| **Jupyter Notebooks**       | The entire process is documented in notebooks, which include narrative explanation and code. This serves both as documentation and as an interactive interface for analysis—e.g., one notebook might demonstrate how to query the repository to retrieve a property trend (like how a certain additive affects viscosity across several formulations). |
| **Visualization and Analysis** | For analysis, we use plotting libraries to visualize properties distributions or correlations. For instance, show a table or bar chart of different formulations vs. a performance metric. We might also use specialized visualization for chemical structures (e.g. RDKit can render structures) if needed to illustrate what chemicals are in a formulation. However, main focus is property data: e.g. using Matplotlib/Seaborn to plot mixture property predictions vs. actual, or network diagrams showing relationships (similar to Project 1). |

### Methodology and Pipeline  
The methodology to build the Chemical & Formulation Property Repository involves collating data from multiple streams and integrating them semantically. The steps are:

1. **Data Ingestion – Pure Properties:** We begin by gathering pure substance data. Using the **Chemicals library** and **PubChem API**, we assemble a table of key properties for a list of substances of interest (the union of all unique chemicals appearing in formulations or of high interest). For example, for each chemical we collect molecular weight, boiling point, toxicity class, etc. This is automated: a Python script takes a list of chemical identifiers (names or CAS) and queries the library/API, building a DataFrame of properties. The script handles issues like synonyms by using CAS registry numbers or InChIKeys as the primary key for each substance[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://pypi.org/project/chemicals/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "11"). All retrieved data is saved (e.g. as `data/pure_props.csv`) for transparency.  

2. **Data Ingestion – Formulation Data:** Next, formulation or mixture data is ingested. If using external datasets (like CheMixHub or literature sources), any provided files (CSV, JSON) are loaded. For instance, CheMixHub might provide a dataset of solvent mixtures with their boiling range; we load that into a DataFrame. Additionally, our *simulated in-house formulation CSV* (with columns like *FormulationID*, *Component1, Component1Amount*, *Component2,...*, *PropertyX*, *PropertyY*) is read. We then have to **normalize ingredient naming** – ensuring that the ingredient names match the keys in our pure substance table. This is where we apply **name-to-identifier resolution**: using RDKit or a lookup to convert ingredient names to canonical identifiers (like an InChIKey or a CAS number) that we use in the pure property dataset. By doing this resolution, each formulation’s components can be linked to the exact chemical nodes from the pure data.  

3. **Data Transformation & RDF Modeling:** With both data sets in hand, we construct an RDF graph to model them. We create individual nodes for each chemical substance (if not already created in Project 1’s dataset, we create anew or will later link them by same IDs). Each property of a pure substance is added as an RDF triple: e.g. `:Acetone :hasBoilingPoint ":56 °C"^^xsd:float` (with proper unit annotation). For formulation entries, we create a node for each formulation (e.g. `:Formulation123`) and link it to its components: `:Formulation123 :hasComponent :Acetone` (perhaps with a blank node or auxiliary node describing the amount of Acetone, e.g. using a n-ary relation pattern to include quantity). We also add triples for the formulation’s overall properties (like viscosity, performance metrics). This step is somewhat complex as it involves designing a small ontology for formulations: for example, representing an **Ingredient** as an entity linking a Chemical and a quantity (to capture percentage or concentration in the mixture). All this is done using **RDFLib** in Python, iterating through the DataFrame rows to generate triples. We ensure to tag each property with its unit and experimental conditions where applicable (for FAIR richness of metadata[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://knowledgebase.nfdi4chem.de/knowledge_base/docs/fair/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "2")).  

4. **Ensuring Interoperability and Standards:** At this stage, we cross-check that the vocabularies used align with standards. For instance, for units we incorporate QUDT or OM (Ontology of Units) so that “°C” or “Pa·s” (for viscosity) are properly referenced, not free text. Chemical substances are ideally identified by a URI derived from a known namespace (if available, e.g. we could use PubChem CID URIs or InChIKey URIs). This means if PubChem has an RDF (PubChemRDF) entry for a compound[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01017-0?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "7"), our chemical node could directly use that URI, linking our property data to PubChem’s graph (this is considered for Project 4 linking). If that’s not directly done now, we at least include the InChIKey as a literal or annotation so linking can be done later.  

5. **Loading and Merging Graph Data:** The triples from this project are then loaded into the graph database. If using the same triple store as Project 1, we either load into a separate named graph (to keep datasets logically separated) or directly merge. Because we use consistent identifiers (like a chemical’s InChIKey or CAS), any identical chemical will automatically merge in the graph DB[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://aws.amazon.com/blogs/database/triple-your-knowledge-graph-speed-with-rdf-linked-data-and-opencypher-using-amazon-neptune-analytics/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "10"). For example, if acetone was also in Project 1’s data (from regulatory info) and identified by InChIKey there, loading Project 2’s triples for acetone’s properties will attach to the same node, enriching it. This illustrates interoperability – using shared keys means the data interlinks without manual intervention[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://aws.amazon.com/blogs/database/triple-your-knowledge-graph-speed-with-rdf-linked-data-and-opencypher-using-amazon-neptune-analytics/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "10").  

6. **Querying & Example Use Cases:** Finally, we perform queries to validate the integrated repository. Some example SPARQL queries and analyses might include: 
   - Querying for all properties of a given chemical (to demonstrate that one can retrieve a “profile” of a substance from the graph). The output would show, say, melting point, flash point, etc., each with units and source reference. 
   - Querying across formulations, e.g. “Find all formulations where Chemical X is a component and list their Y property” – for instance, find all paint formulations containing a certain solvent and compare their drying times. This uses the graph relationships: `?formulation :hasComponent ?chem . FILTER(?chem = :Acetone)` to filter formulations containing acetone, then get `?formulation :dryingTime ?t`. We then possibly visualize or tabulate the results to see how acetone amount correlates with dryingTime. 
   - Another example: “List formulation components that most frequently appear together with Substance Y” – essentially a network analysis query to find if certain ingredients are commonly paired, which could hint at functional groupings in formulation design. 
   
   The results from SPARQL can be processed (in Python) to generate a simple analysis, like outputting the top 5 most common co-ingredients with acetone in our dataset, etc.

7. **Documentation & Reproducibility:** The project includes a **Setup Guide** (in README) instructing how to install required libraries (including RDKit, the Chemicals library, etc.), how to configure the graph database connection, and how to run the ingestion notebooks. We provide sample data (or references to them) so that a user can run the pipeline with provided example datasets. Tests or data validation checks are in place at certain points (for example, verifying that every formulation component was successfully matched to a known chemical and flagged if not).

### Expected Outcomes and Findings  
Project 2 results in a functional **Chemical Properties Repository** that is valuable for chemical engineers, formulators, and data scientists:

- **Comprehensive Property Database:** A knowledge graph that not only stores basic properties of individual chemicals, but also contextualizes them in mixtures/formulations. One concrete outcome is that any given chemical in the system has a richly annotated node: e.g. “Ethanol” will have links to its boiling point, flash point, toxicity, and also links to any formulation nodes in which it participates. This showcases **reusability** of data: the same ethanol data is reused in multiple contexts (which aligns with FAIR reuse principles) instead of siloed copies[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1").
- **Standardized Metadata and Units:** All property data is associated with consistent units and metadata, making comparisons straightforward. For instance, if some boiling points came in as ℃ and others as K, the pipeline normalized them (to K internally, for example). Querying the graph for boiling points will yield values with unified units. This standardization addresses a common challenge in data integration – by using formal representations and ontologies, the data is self-describing and unambiguous[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1").
- **Insights into Formulation Design:** With the integrated data, one can derive insights such as which properties of components most influence a formulation’s outcome. Suppose this repository included a set of drug formulation data with dissolution rates. A Data Scientist could query: “For all formulations of Drug A, retrieve the particle size of Drug A and the dissolution rate” – then perform a correlation analysis. The expected finding might be that smaller particle size (a property of the drug substance) leads to faster dissolution in formulations, quantifying a known trend. The repository thus aids in *data-driven formulation optimization*. 
- **Case Study Results:** The documentation includes a small case study, e.g., analyzing the **battery electrolyte dataset** from CheMixHub: The pipeline might show how to retrieve the relationship between electrolyte composition and ionic conductivity. The expected result could illustrate that certain solvent mixtures yield higher conductivity, and a SPARQL query can pinpoint which mixture had the highest measured value, including its composition. 
- **Increased Efficiency for Scientists:** By demonstrating how to centralize property data, the project underlines efficiency gains. Historically, a formulator might search through separate documents or databases for each ingredient’s info. Here, with a SPARQL or a simple query interface, all needed properties are one query away, and even complex questions (like cross-formulation comparisons) can be answered in seconds if the data is loaded. This echoes the idea that reusing well-curated data makes researchers’ tasks easier[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://knowledgebase.nfdi4chem.de/knowledge_base/docs/fair/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "2").
- **Reproducibility and Extension:** The output repository is reproducible, meaning if new property data becomes available (say a new assay for a chemical’s biodegradability), it can be added following the documented procedure. The project can be extended by linking to external knowledge resources: e.g. linking a chemical’s property node to its entry in Wikidata or PubChemRDF, effectively enhancing the graph with external knowledge. 

In summary, Project 2 delivers a robust, semantically-enriched property database that serves as a reference for both individual chemicals and complex formulations. It sets the stage for linking with internal experimental data (Project 3) and ultimately the cross-dataset integration (Project 4) by ensuring all data uses consistent identifiers and formats.

---

## Project 3: In-House & Synthetic Manufacturing Data Management

### Objectives  
Project 3 addresses the integration of **in-house experimental and manufacturing data** – i.e., data generated within an organization’s R&D or production processes – into a structured, searchable knowledge system. The focus is on **synthetic chemistry and manufacturing records**. Objectives include:

- **Capturing Internal Knowledge:** Develop a pipeline to ingest data from laboratory notebooks, batch records, or manufacturing logs (e.g. reaction conditions, yields, process parameters, quality metrics) into a structured format. This often means parsing Excel sheets or database exports used internally and converting them into a standardized schema for reactions/processes[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13").  
- **Applying FAIR Principles Internally:** Just because data is internal doesn’t exempt it from FAIR. We aim to make internal data *findable* (assign unique IDs to experiments, use indexes), *accessible* to the team (via a knowledge graph or database interface, rather than buried in reports), *interoperable* (using standard chemical identifiers and ontologies so it can link with external data), and *reusable* (rich metadata, provenance, and documentation so future scientists can trust and reuse old experiment data)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13")[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1").  
- **Semantic Modeling of Reactions/Processes:** Use an RDF/graph approach to model a **chemical reaction or manufacturing process**. Each reaction will be an entity with relationships to reactants, products, catalysts, conditions (temperature, pressure, etc.), and results (yield, purity). By modeling this, we enable complex queries like “find all experiments that produced Compound X” or “what were the highest yields achieved for reaction type Y and under what conditions?”  
- **Integration with External Knowledge:** Prepare the data so that it can be linked with external datasets (from Projects 1 & 2). For example, internal data’s chemical entities should align with the same identifiers used in the public datasets so that hazards or properties can be cross-referenced.  
- **Clear Execution Guide:** Provide instructions for how an organization could implement this: from exporting data out of an Electronic Lab Notebook (ELN) or process database, through transformation, to loading into a graph database. It should also note security or confidentiality steps (e.g. how to keep the system internal or anonymize as needed) though the implementation is focusing on the technical pipeline.

### Data Sources and Nature of Data  
This project uses **simulated in-house data** (to avoid proprietary issues, we create a representative dataset). Key data elements and their assumed sources: 

- **Reaction Dataset (In-House ELN Exports):** For instance, a collection of organic synthesis experiments recorded in an ELN. We simulate this as a CSV or JSON where each entry has fields like ReactionID, Date, Experimenter, Reactants (with amounts), Solvent, Temperature, ReactionTime, Product, Yield, Remarks. This mimics an export from an ELN or a LIMS (Laboratory Information Management System). Each reactant and product is given by name or structure. Yields and other results are numeric. We might include a few dozen example reactions covering different types (to illustrate versatility).  
- **Manufacturing Process Data:** Optionally, a separate set of data representing a scaled-up process (e.g. a production batch record). This could include fields like BatchID, Product, Equipment, StartTime, EndTime, parameters (pH, stirring rate, etc.), result metrics (purity, quantity produced). This shows that the pipeline can handle both lab-scale and plant-scale data with similar schemas.  
- **Reference Ontologies/Ontological Data:** Not data per se, but for modeling purposes we incorporate existing standards like the Reaction Ontology (RXNO for reaction types), CHEBI for chemical entities, and possibly PROV-O (W3C Provenance Ontology) to encode provenance of data (like which chemist performed it, which lab). We also use the ORD (Open Reaction Database) schema concepts as a guide for data modeling[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://open-reaction-database.org/about?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "14"). The ORD schema is JSON-based, but we can align with its concepts (reaction, components, outcome) for consistency with industry practice.  
- **Internal Metadata:** Additional contextual data such as user info, project codes, etc., which might be present in an enterprise scenario. We simulate minimal metadata (e.g., Experimenter name or an internal project reference) just to show that it can be included and could be used for access control or tagging later.

Because internal data is not from the web, the “sources” here are hypothetical internal sources, but the pipeline will treat them as input files. We ensure to cite best practices from literature on how to manage ELN data to guide this integration[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13").

### Tools & Technologies  
| Tool / Technology             | Usage in Project 3                                              |
|-------------------------------|------------------------------------------------------------------|
| **Python (pandas, openpyxl)** | Used for reading internal data files. If the ELN export is in CSV/Excel, `pandas` will parse it. If the data is in a proprietary format, one might need to use an API – since we simulate, we stick to CSV/JSON. Data cleaning (trimming spaces, converting data types) and initial analysis (like counting entries) are done with pandas. |
| **Regular Expressions / NLP** | If unstructured text needs parsing (e.g. free-text experimental descriptions), regex or simple NLP can extract structured info. For example, a text “Yield: 85%” is parsed to a yield field = 85. In our dataset, we likely already have structured fields, but we include tools to handle slightly messy inputs (common in real lab notes). |
| **RDFlib and Custom Ontology** | We build an ontology for “Reaction” data. Each Reaction (or Process) is an RDF individual of class `ReactionExperiment`. We define properties like `hasReactant`, `hasProduct`, `hasCondition`, `yieldPercent` etc. For each reactant or product, we reference the chemical by its URI (linking to the chemical node used in Projects 1 & 2). Reaction conditions can be represented either as literal data or as entities (e.g. a `Condition` node with property type=Temperature and value=50°C). We likely simplify by using literal values for numeric conditions and a controlled vocabulary for types of conditions. RDFlib is used to create all these triples from the DataFrame. We incorporate **PROV-O** to link that a Reaction was performed by a Person (we can create Person nodes or simply attach as a literal “performedBy”). PROV-O’s `wasAssociatedWith` or similar can capture that relation to an experimenter[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13"). |
| **Graph Database (RDF Store)** | The internal data graph is uploaded to the triple store. If confidentiality is a concern, this graph could remain isolated or on a secure instance. For our case, we assume a local triple store where we have full control. By storing it, we get the ability to query internal data by SPARQL just like the other datasets. The triple store ensures even if internal data model is custom, it’s still queryable with SPARQL and linkable. |
| **SPARQL and Analytics Tools** | We use SPARQL to query the reaction data for patterns (e.g. find average yield of a reaction type). We might also use Python for further analysis; for example, after querying all reactions for a certain product, use Python to do a time-series if data has dates, or to plot yield vs. an experimental parameter. Visualization libraries will be used for any charts (e.g. yield distribution histogram). |
| **Security/Provenance Tools** | While not a separate tool, we document measures for data governance: e.g., how to assign access rights (though not implemented in code, perhaps note that one could use named graphs to separate sensitive data), and how each triple related to internal data can carry provenance (like an experiment reference or a DOI if published). In our RDF, we might include a triple linking to a lab notebook reference ID or a report PDF (URL) for traceability. |

### Methodology and Workflow  
1. **Exporting Internal Data:** (Simulation) – We imagine the chemists export their ELN entries as a CSV. The first step is getting this data. In practice, many ELNs have **APIs** or export functions[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13"). We cite an example: Chemotion ELN can export to structured data, or a generic ELN might have an API to fetch records[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13"). Our pipeline starts with reading the provided `internal_reactions.csv`. This file contains structured fields as described above. We also load any batch process data similarly.  

2. **Data Cleaning & Harmonization:** Internal data might use inconsistent naming (one experiment might call a chemical “Acetone”, another “propanone” or a code). We apply the same strategy as Project 2: use a resolver to unify chemical names. We cross-reference the chemicals appearing in reactions with our master list of chemicals from Projects 1 & 2 (by name or CAS). If a chemical isn’t found, we attempt to identify it via external services (e.g., using PubChem name search). This ensures that each reactant/product in the internal data can be linked to a unique chemical entity. We assign internal IDs for each reaction (if not already present) so we have a stable reference. We also ensure numeric fields like yield are numeric (and possibly scale them to 0–1 vs percentage for consistency if needed).  

3. **Ontology Mapping:** Before writing RDF, we formalize how to represent the data. For example, we decide that a ReactionExperiment will have the following structure in RDF:  
   - A unique URI like `:RXN123` (maybe derived from ReactionID).  
   - Triples linking to reactant chemicals: `:RXN123 :hasReactant :Chemical_Acetone` (possibly with an intermediate blank node if we want to include amount). Given complexity, we might encode amount as a property on the reactant node: e.g. `:RXN123 :hasReactantAmount_Acetone "5 mL"`. Alternatively, use the ORD style: create a node `:RXN123_Reactant_1` of type `ReactionComponent` with `componentChemical = Acetone` and `amount = 5 mL`. We decide on one approach and stick to it, documenting it in the code comments.  
   - Similar structure for products (e.g. `:hasProduct`).  
   - Conditions: We introduce properties like `:temperature` (or use an ontology if available, e.g. the Ontology of Experimental Conditions). For simplicity: `:RXN123 :temperature "50 °C"^^xsd:string` (or xsd:double with a unit annotation). We do this for key conditions present in the data (time, temp, etc.).  
   - Outcome: `:yieldPercent` as a data property on the reaction node. Also perhaps `:outcome` could link to a category like Success/Failure if that were in data (some databases mark if reaction failed, but let’s assume most have yields).  

   We leverage **existing ontologies** whenever possible: for example, if RXNO (the Reaction Ontology) has a term for “Suzuki Coupling Reaction”, we could tag ReactionExperiment instances with a property `rxno:type rxno:000001` (just as an example) to categorize reaction type. If our data includes a text field for reaction type, we map it to an RXNO term. This makes our internal data interoperable with others' by reaction taxonomy[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://open-reaction-database.org/about?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "14").

4. **RDF Conversion:** Now using RDFLib, we iterate through each internal record and create the structured triples according to the above mapping. Each reaction becomes a node in the RDF graph with all its relations. We also create Person nodes for chemists (e.g. `:Chemist_JaneDoe` of type `foaf:Person`), and link `:RXN123 prov:wasAssociatedWith :Chemist_JaneDoe` to record who did it[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13"). If dates are present, we use `prov:startedAtTime` or similar to attach timestamps. This embedding of provenance and context aligns with best practices (it’s noted that well-annotated and standardized data increases reusability of ELN records[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13")). We also attach a provenance reference literal like `prov:wasDerivedFrom "LabNotebook45 Page 17"` to trace back to the original note.  

5. **Integration into Graph DB:** We import the internal data RDF into the graph database. At this point, our graph database has (at least) three categories of information: public regulatory data (Proj1), property data (Proj2), and internal experimental data (Proj3). Because we used consistent chemical identifiers, the reactant/product links from ReactionExperiments should connect to the same `ChemicalSubstance` nodes that have properties and regulatory info. This is the beginning of an integrated system: for example, the chemical node for “Acetone” now has relationships to public data (source, hazard perhaps), physical properties (boiling point, etc.), and appears in one or more ReactionExperiments as a reactant. We confirm these links by running a test query after loading: pick a known chemical from the internal data and query the graph for all triples involving it, to see that indeed it links to internal and external info.  

6. **Querying Internal Data:** With all internal experiments in the knowledge graph, we showcase queries that leverage this new dataset:
   - *Experiment Search:* “Find experiments that produced Compound X”. In SPARQL: given Compound X’s URI, find ReactionExperiments where `:hasProduct CompoundX`. The result returns Reaction IDs which we then lookup for yields or conditions. This can help someone see all ways Compound X was made in the organization. 
   - *Conditions vs Outcomes:* “List all reactions of type Y (e.g., esterification) and show yield vs temperature”. This requires that we tagged reaction type (which we did via an ontology or even a simple `reactionType` property). SPARQL collects all reaction instances with that type, and returns their yield and temperature data. We then could plot yield against temperature to see if higher temperature correlates with better yield for that reaction type in our internal history.
   - *Process Improvement:* “What’s the best yield achieved for Product Z and what were the conditions?” This query might sort reactions by yield for a given product and pick the top one, returning that reaction’s conditions. This directly gives insight into optimal conditions found historically, useful for process chemists.
   - *Meta-analysis:* We can also query usage frequency of reagents: e.g., “What are the top 5 reagents by count of usage in all experiments?” This is a SPARQL aggregation counting `?reagent occurrences`. It reveals which reagents are most commonly used—perhaps showing solvents like Dichloromethane or catalysts like Pd appear frequently, which might interest procurement or standardization efforts.

   These queries demonstrate how having the data in a knowledge graph allows flexible questions. Traditional ELNs often allow searching by text, but a structured graph enables complex, structured queries across multiple criteria (substance, reaction type, outcome), highlighting the benefit of semantic integration.

7. **Documentation:** We provide extensive documentation on how the data was mapped (including a dictionary of CSV columns to ontology terms). The code and ontology schema (perhaps as a small OWL/Turtle file defining classes/properties for ReactionExperiment) are included. We also include guidance such as: if the company had multiple ELNs, how to merge data (pointing to research on bridging ELNs[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13")). We cite that **ELNdataBridge** work which shows it’s feasible to link disparate ELN systems via a common adapter[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13"). Our approach similarly creates a neutral representation (RDF) that any ELN data can be mapped into, thereby facilitating data exchange and central querying.

### Expected Outcomes  
Project 3 delivers an internal knowledge graph of R&D data. The outcomes and benefits are:

- **Knowledge Preservation:** All internal experiments are now captured in a **queryable knowledge base**. This means institutional knowledge (often lost in reports or when employees leave) remains findable. For example, a chemist years later can query if a certain reaction was attempted before and retrieve the detailed context, instead of digging through lab notebooks. This aligns with improving transparency, traceability, and reproducibility of experiments[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13").
- **FAIR Internal Data:** By assigning unique identifiers to experiments and using common chemical identifiers, we made the data more *findable* (e.g. ReactionIDs are indexable, chemicals are linked by InChIKey) and *interoperable* (we can directly connect this with external data because of shared IDs and ontologies)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1"). Rich metadata like who performed an experiment and when (provenance) and attaching references to original records help with *reusability*, as future users trust and understand the context[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1"). Although the data stays internal, it is now structured enough that if portions could be published (say in a paper’s supporting info), it would already adhere to FAIR principles.
- **Improved Query Capability:** The ability to run complex queries on internal data can drastically improve decision-making. For instance, finding all instances of a failed reaction could prevent repeat attempts, or aggregating yield data could highlight which conditions systematically lead to better outcomes. It’s essentially turning a collection of experiments into a **knowledge graph** that can answer questions, similar to how big companies leverage internal data lakes. Our demonstration queries likely yield meaningful visualizations – e.g., a chart from the earlier example might show that beyond 80°C, yields plateau or drop, giving insight into reaction behavior. 
- **Integration-Readiness:** The internal dataset is prepared to be linked with external. For example, each chemical node here can be augmented with regulatory info (from Project 1) – we could easily enrich internal reaction data with hazard warnings for reagents (helping lab safety assessments). Or link with property data (Project 2) – e.g., fetch boiling point of a solvent used in an experiment to see if that affected an outcome. While these links are part of Project 4, the groundwork is laid: internal data isn’t in a siloed format but already uses the global language of RDF and overlapping ontologies.
- **Reproducible Template:** The project provides a template for any lab interested in **data digitalization**. It illustrates how to go from raw ELN data to a structured database, highlighting tools needed (and referencing real solutions like ORD schema or ELN APIs). It also demonstrates handling heterogeneity: labs may use different terms, but mapping to a common model resolves that. This is in line with industry trends to make lab data more standardized and sharable[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13").
- **Potential Findings:** On a small scale, our synthetic dataset might show some patterns (that we intentionally or randomly put). For example, we might note that “reactions done in solvent X had on average 10% higher yield than those in solvent Y” or “Most scale-up batches reached >95% purity except one outlier – upon investigation, that outlier’s conditions differ significantly (documented in the graph).” These pseudo-findings demonstrate the kind of insight one could get. We would include one such example in the report narrative.

In conclusion, Project 3 brings internal R&D and manufacturing data to life by structuring it in a knowledge graph. It ensures no piece of data is stranded and sets the stage for **linking across datasets** in Project 4, where the true power of combining public and private data will be seen.

---

## Project 4: Linking Across Datasets – Integrated Knowledge Graph

### Objectives  
Project 4 is the culmination of the previous projects, focusing on **linking across all datasets** to create an integrated **“enterprise knowledge graph”** of chemical information. The themes are interoperability and cross-domain queries. Key objectives:

- **Unify External and Internal Data:** Merge the graphs from Projects 1, 2, and 3 (and potentially additional sources) so that all information about a chemical or process can be accessed in one place. Ensure that public reference data, property data, and in-house experiment data are not isolated silos but interconnected pieces of a single knowledge network[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://aws.amazon.com/blogs/database/triple-your-knowledge-graph-speed-with-rdf-linked-data-and-opencypher-using-amazon-neptune-analytics/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "10").  
- **Implement Linking Strategies:** Use semantic web techniques like owl:sameAs, common identifiers, and ontology alignment to formally link identical entities across datasets. For example, link a chemical’s internal URI to its PubChem URI, or declare two names as the same chemical via owl:sameAs if needed[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://aws.amazon.com/blogs/database/triple-your-knowledge-graph-speed-with-rdf-linked-data-and-opencypher-using-amazon-neptune-analytics/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "10"). Create cross-references such that, for instance, an internal substance or product is connected to its regulatory information from ECHA and its properties from PubChem.  
- **Enable Cross-Dataset Queries:** Demonstrate complex queries that were not possible before integration. For instance, query that involves regulatory + internal data (“List in-house reactions involving substances that are classified as hazardous”) or property + internal data (“For each in-house experiment involving a high-boiling solvent, list the yield, to see if high boiling point correlates with yield improvement”). The objective is to show the *added value* of linking: new insights and simplified workflows for end-users.  
- **FAIR Compliance and Accessibility:** Finalize the knowledge graph as a FAIR resource. This involves documenting the entire schema, providing a SPARQL endpoint or API for it, and perhaps packaging the data (minus sensitive parts) as open data (if possible) or at least well-documented internal data. The integrated system should be *findable* (indexed, with searchable metadata across datasets), *accessible* (through standard protocols like SPARQL/HTTPS)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1"), *interoperable* (it’s built on RDF and shared vocabularies), and *reusable* (clear licensing for public parts, provenance for each data point from its source)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1").
- **Scalability and Maintenance:** Discuss how the system can be maintained/grew. For example, if new data sources come (new regulatory list, new experiments), how they can be added with minimal friction thanks to the design. Also consider performance (large graph queries) and using graph database features (like inference or full-text search) to enhance linking.

### Approach to Linking and Integration  
To achieve the above, we outline the integration strategy employed:

- **Identifier Alignment:** The cornerstone of linking data is having common identifiers. In Projects 1–3, we attempted to use common keys (like CAS numbers, InChIKeys) for chemicals, and common terms for other entities where possible. In this project, we enforce alignment: if, for example, the internal data uses a different ID for a chemical than the external (which can happen if we didn’t fully unify or if multiple URIs exist), we reconcile them. One approach is using `owl:sameAs` to state two URIs refer to the same thing. However, we have largely avoided duplicate URIs by design. Another alignment is linking to *external linked data*: e.g., link each chemical node to a DBpedia or Wikidata entry (which are linked open data hubs). We may include owl:sameAs or `schema:sameAs` links from our chemical to its Wikidata URI, for instance. This not only validates identity but also brings in a wider linked data context (though not required by the project, it demonstrates extending integration beyond our collected datasets).  

- **Ontology Alignment:** Where we used custom ontology terms in one project and a different term in another for the same concept, we unify. For instance, if Project 2 had a property “FlashPoint” and Project 1 used a generic property “hasProperty” with a subproperty “flammability”, we align these by making “FlashPoint” a sub-property or same-as the one used elsewhere. We refine our ontology so the final graph has a coherent schema. In practice, we merge the ontologies or just ensure queries account for both. We keep the ontology mapping documented so that users querying the graph know, for example, that `ex:boilingPoint` from one source is the same as `cheminf:0002` (hypothetical) from another source, if we had that scenario.

- **Use of Graph Database Features:** Our graph database likely supports some form of reasoning or aliasing. We might use that to materialize sameAs links or to use property paths in SPARQL to traverse equivalences. Additionally, if using a property graph perspective (some graph DBs like Neo4j can import RDF and also use Cypher), we could do cross-dataset queries in Cypher. However, SPARQL is sufficient. We ensure the triple store has indexes on the common keys (like InChIKey index) for performance.

- **Data Volume & Quality:** We integrate at least a few hundred chemicals and dozens of reactions in the example, which is manageable. In a real scenario, linking a million chemicals (EPA data) with thousands of experiments might hit performance issues, but graph databases like Amazon Neptune or GraphDB are designed for millions of triples[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://aws.amazon.com/blogs/database/triple-your-knowledge-graph-speed-with-rdf-linked-data-and-opencypher-using-amazon-neptune-analytics/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "10"). We mention that scaling considerations might involve partitioning or using hybrid queries (some queries on relational side if needed), but since this is a portfolio project demonstration, our scale is moderate.

### Integrated Workflows and Queries  
With the integrated knowledge graph in place, we highlight some **cross-cutting workflows** that become possible. The table below summarizes a few such workflows and how the linked data enables them:

| Cross-Dataset Query/Workflow                      | Description & Outcome                                                  |
|---------------------------------------------------|------------------------------------------------------------------------|
| **Safety Check for Lab Experiments**              | *Query:* Find all internal ReactionExperiments where any reactant or product is classified as a hazardous substance (e.g., carcinogen or environmentally toxic) according to regulatory data. *Implementation:* A SPARQL query traverses from Reaction -> hasReactant -> Chemical -> hasHazardClassification. If the hazard classification matches a certain criterion (like GHS codes H350 etc. from Project 1 data[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.prc.cnrs.fr/reach/en/databases.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "5")), it selects that reaction. *Outcome:* The query returns a list of Reaction IDs along with the hazard details, alerting that these experiments involve particularly dangerous chemicals. This showcases linking internal experiments to public regulatory knowledge. Without integration, a safety officer would have to manually cross-check each chemical against regulatory lists; now it’s automated. |
| **Property-Driven Process Optimization**          | *Query:* For each internal reaction, pull in boiling point of the solvent used (from Project 2’s property repository) and the reaction yield (from Project 3), then analyze if higher boiling solvents gave better yields. *Implementation:* SPARQL can chain: Reaction -> hasReactant (solvent) -> hasProperty (boiling point) and also Reaction -> yield. It gathers those data. We then plot yield vs solvent boiling point. *Outcome:* Perhaps the plot shows a trend that solvents with boiling point above 100°C give consistently higher yields for a certain reaction type. This could guide chemists in solvent selection. It directly leverages the linked property data and internal results. |
| **Find External Info for Internal Compounds**     | *Query:* For each key product made in-house (from Project 3), retrieve any known regulatory or reference data (from Project 1). *Implementation:* Taking a product’s InChIKey, query the graph for any nodes with that InChIKey that have external attributes like an ECHA registered name or toxicity data. *Outcome:* The output might show that an internal product (say a novel compound) has no hits (meaning it’s truly new), whereas another matches a known substance that has, for example, a REACH registration tonnage band or a known DNEL (Derived No-Effect Level)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.prc.cnrs.fr/reach/en/databases.html?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "5"). That information might be crucial for regulatory planning if the company wants to scale production. This highlights how linking to public data can inform decisions on internal compounds (like regulatory hurdles). |
| **Global Substance Overview Dashboard**           | *Query:* Construct a dashboard for a given chemical that pulls all connected info: regulatory status, physical properties, and occurrences in internal processes. *Implementation:* A set of queries filter all triples related to a chemical node. We can gather: synonyms, hazards, boiling point, any formulation it’s in, any reaction it’s used in, etc. *Outcome:* This comprehensive profile could be presented in a tabular or graphical format. It is essentially a one-stop summary that would typically require searching multiple databases (safety data sheets, internal reports, etc.)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://api.store/eu-institutions-api/european-chemicals-agency-api?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "6"). The integration fulfills the vision of a unified chemical information system for the organization. |
| **Consistency Checks & Data Validation**          | *Workflow:* Because data from different sources is linked, we can run consistency checks. For example, if a public data source says Chemical X’s melting point is 50°C but internal lab measurements found 60°C, the graph highlights this discrepancy. *Implementation:* SPARQL query that compares values if both exist for a property from two sources (could use provenance info to distinguish sources). *Outcome:* If inconsistencies are found, they can be flagged for review—maybe the lab measurement was under different conditions or the public data is wrong. This use shows how linking not only answers queries, but also helps validate and enrich data quality by comparison across sources. |

These linked scenarios demonstrate qualitative improvements: better safety oversight, knowledge reuse, and data-driven decisions, all enabled by combining the datasets.

### Tools & Final Setup  
The final integrated system uses the same tools but orchestrated together:

- We likely run a final **graph merge script** that ensures all data is loaded and performs any final linking (like adding owl:sameAs triples for certain known equivalences).
- We configure a SPARQL endpoint (which could be the graph DB’s built-in endpoint) for end-users. We document some example SPARQL queries (as shown above) in a user guide.
- We discuss deployment: e.g., running the graph database as a service (maybe Dockerized for ease), so that others (data scientists, apps) can query it anytime. We mention that with the data integrated, one can even build a simple web application on top. For example, a web UI where you input a chemical name and it presents all info (this could use the SPARQL endpoint behind the scenes). While building the UI is outside scope, we outline that the data foundation is ready for such front-end integration.

- If applicable, we mention using reasoning: e.g., if we set up some OWL axioms (like inferring a reaction involves a hazardous chemical -> the reaction is hazardous), a reasoner could automatically tag those reactions. Many graph DBs allow toggling a reasoner to materialize such inferences.

### Expected Outcomes and Impact  
By completing Project 4, we achieve a **fully integrated knowledge graph** which is the ultimate goal of a FAIR data infrastructure in chemistry R&D. The expected outcomes and their significance are:

- **Holistic Knowledge Graph:** A single graph database now links regulatory knowledge, fundamental properties, and experimental results. In terms of scale, suppose we integrated 100k substances from ECHA, property data for say 5k substances, and 200 internal experiments – the final graph might contain millions of triples, but still navigable. The success is evident when a single query can seamlessly traverse between what was previously disparate data worlds. This is essentially the “digital twin” of the organization’s chemical knowledge. Such integration is advocated in literature as the future of cheminformatics data management[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01017-0?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "7")[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://open-reaction-database.org/about?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "14").
- **Demonstrated FAIR Data Use:** The integrated dataset itself can be considered **FAIR**. For instance, each chemical is a node with a URI that could be dereferenced (if we set it up) – Findable and Accessible. Data uses shared vocabularies and references between data (Interoperable)[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1"). The provenance of data is stored (each triple from external source can carry an annotation of origin, and internal data has prov links) – this plus documentation of the schema and context makes it Reusable[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://www.go-fair.org/fair-principles/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "1"). We can cite how this meets each FAIR letter, showing that even internal & external mashup can be FAIR. 
- **New Insights & Efficiency:** The integrated queries likely produce **actionable insights** that were not obvious before. For example, the safety query might lead the organization to substitute a hazardous reagent with a safer one if they realize how often it’s used. Or the optimization query could save time by pointing to ideal conditions from past data. This integrated approach echoes real-world applications where knowledge graphs help in decision support (for instance, a 2024 study combined knowledge graphs and AI for hazardous chemical management, indicating the value of linked information for reusing knowledge).
- **Portfolio-Ready Case Studies:** The project, as documented, serves as an impressive portfolio piece. It shows end-to-end capability: data engineering, semantic modeling, database deployment, and analysis. Each part is backed by current best practices (with sources cited from chemical informatics and data management domains), convincing any reviewer that the approach is grounded in expert recommendations. For instance, the use of an open reaction schema (ORD) and linking ELN data via APIs is something emerging in 2025[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://open-reaction-database.org/about?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "14")[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01024-1?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "13"), showing this project is at the cutting edge of lab data integration.
- **Scalability & Future Extensions:** The outcomes also include a discussion on future work. For example, integrating literature data (text-mined relationships from publications) into the graph (one could use the named entity co-occurrence RDF approach like PubChemRDF did[43dcd9a7-70db-4a1f-b0ae-981daa162054](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-025-01017-0?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054 "7"), to link our compounds to literature). Or hooking the graph up to a machine learning pipeline (e.g. using the data to train a model for yield prediction – which could then feed back into the system as new knowledge). The project architecture is flexible for such extensions, making it a robust base for continuing development.

In sum, Project 4 yields an **integrated chemical information system** where everything is interlinked. The successful linking across datasets not only validates the technologies (Python, RDF, graph DB) in a realistic scenario, but also vividly demonstrates the principle that “the whole is greater than the sum of its parts” – the combined knowledge graph empowers users to ask and answer complex questions, fueling better research and development outcomes. With comprehensive documentation and reproducible steps, this integrated project stands as a capstone case study in a portfolio, highlighting skills in data integration, semantic modeling, and applying FAIR principles to solve real-world problems in chemical data management. 


